{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2000 # 에포크\n",
    "print_every = 100 # 출력 사이클\n",
    "plot_every = 10   # 플롯 사이클\n",
    "chunk_len = 200   # 청크 길이 \n",
    "hidden_size = 100 #\n",
    "batch_size = 1    # 배치크기\n",
    "num_layers = 1    # 레이어 수 \n",
    "embedding_size = 70 # 임베딩 벡터 크기\n",
    "lr = 0.002        # 학습률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\r",
      "\u000b",
      "\f",
      "\n",
      "num_chars =  100\n"
     ]
    }
   ],
   "source": [
    "# import 했던 string에서 출력가능한 문자들을 다 불러옵니다. \n",
    "all_characters = string.printable\n",
    "\n",
    "# 출력가능한 문자들의 개수를 저장해놓습니다.\n",
    "n_characters = len(all_characters)\n",
    "print(all_characters)\n",
    "print('num_chars = ', n_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 1115393\n"
     ]
    }
   ],
   "source": [
    "file = unidecode.unidecode(open('./data/input.txt').read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "labour and effect one thing specially.\n",
      "\n",
      "GREMIO:\n",
      "What's that, I pray?\n",
      "\n",
      "HORTENSIO:\n",
      "Marry, sir, to get a husband for her sister.\n",
      "\n",
      "GREMIO:\n",
      "A husband! a devil.\n",
      "\n",
      "HORTENSIO:\n",
      "I say, a husband.\n",
      "\n",
      "GREMIO:\n",
      "I say,\n"
     ]
    }
   ],
   "source": [
    "# 랜덤 청크 ( 길이: chunk_len) - 텍스트의 랜덤한 부분 로딩\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "print(random_chunk())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36, 37, 38, 13, 14, 15])\n"
     ]
    }
   ],
   "source": [
    "# 데이터 to tensor (index)\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor\n",
    "\n",
    "print(char_tensor('ABCdef'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤한 파트의 데이터 생성\n",
    "# Eg. Hello => 입력: Hell, 타겟: ello\n",
    "def random_training_set():    \n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, mode):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = 1\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.encoder = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        if mode == 'basic':\n",
    "            self.rnn = nn.RNN(self.embedding_size,self.hidden_size,self.num_layers)\n",
    "        elif mode == 'LSTM':\n",
    "            self.rnn = nn.LSTM(self.embedding_size,self.hidden_size,self.num_layers)\n",
    "        self.decoder = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        out = self.encoder(input.view(1,-1))\n",
    "        out,hidden = self.rnn(out,hidden)\n",
    "        out = self.decoder(out.view(batch_size,-1))\n",
    "        return out,hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        if self.mode == 'basic':\n",
    "            return hidden\n",
    "        if self.mode == 'LSTM':\n",
    "            cell = torch.zeros(self.num_layers,batch_size,self.hidden_size)\n",
    "            return hidden, cell\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(n_characters, embedding_size, hidden_size, n_characters, num_layers,'basic')\n",
    "rnn_lstm = RNN(n_characters, embedding_size, hidden_size, n_characters, num_layers,'LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
    "rnn_lstm_optimizer = torch.optim.Adam(rnn_lstm.parameters(), lr=lr)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 문자(start_str)로 시작하는 길이 200짜리 모방 글을 생성하는 코드입니다.\n",
    "def test(model):\n",
    "    start_str = \"b\"\n",
    "    inp = char_tensor(start_str)\n",
    "    hidden = model.init_hidden()\n",
    "    x = inp\n",
    "\n",
    "    print(\"Start string: \", start_str,end=\"\")\n",
    "    print(\"\\nPredicted string: \")\n",
    "    \n",
    "    for i in range(200):\n",
    "        output,hidden = model(x,hidden)\n",
    "\n",
    "        # 여기서 max값을 사용하지 않고 multinomial을 사용하는 이유는 만약 max 값만 쓰는 경우에\n",
    "        # 생성되는 텍스트가 다 the the the the the 이런식으로 나오기 때문입니다.\n",
    "        # multinomial 함수를 통해 높은 값을 가지는 문자들중에 램덤하게 다음 글자를 뽑아내는 방식으로 자연스러운 텍스트를 생성해냅니다.\n",
    "        output_dist = output.data.view(-1).div(0.8).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        predicted_char = all_characters[top_i]\n",
    "\n",
    "        print(predicted_char,end=\"\")\n",
    "\n",
    "        x = char_tensor(predicted_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tensor([2.1642], grad_fn=<DivBackward0>) \n",
      "\n",
      "Start string:  b\n",
      "Predicted string: \n",
      "est,\n",
      "And my liffing and murb,\n",
      "And what of tha sthe fi that for speane of she my over star whom bear of be lave his dear;\n",
      "sward, is you wor and ied my phander harper:\n",
      "Ang, at litten th\n",
      "And amdem ther b\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8505], grad_fn=<DivBackward0>) \n",
      "\n",
      "Start string:  b\n",
      "Predicted string: \n",
      "lere me were hear in nod ovill thee this gorey.\n",
      "\n",
      "CORANICE:\n",
      "Thet luctiens.\n",
      "\n",
      "Toree I goldull grourd muct and deartanes lotter ap art fell, I condersher Vouces;\n",
      "Have gous this for monsty ho siil not grou\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0188], grad_fn=<DivBackward0>) \n",
      "\n",
      "Start string:  b\n",
      "Predicted string: \n",
      "est,\n",
      "Shy enters on will senteant, lord our with me to some but matter; le jood the deast be seantyen o mord frother durnes, of ik the I yet me, is wordeant sownobles,\n",
      "Beacomafest so the lead the kepre\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9188], grad_fn=<DivBackward0>) \n",
      "\n",
      "Start string:  b\n",
      "Predicted string: \n",
      "e.\n",
      "\n",
      "DUKE CCARDY OF EONE:\n",
      "Kess,\n",
      "And when hem\n",
      "roud, me dear weri:\n",
      "Seen be at have tome arother fore\n",
      "I well you we me limdst the be distss tho but you be to sourt, a were ynou ond thit our pamgowofed tui\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0325], grad_fn=<DivBackward0>) \n",
      "\n",
      "Start string:  b\n",
      "Predicted string: \n",
      "ut of whose, could that attle soll would I'll ar, all the paotere I bleat him, feres repect of than: a not!\n",
      "Thong as you vaitice,\n",
      "Thee,\n",
      "I fack chall and do\n",
      "He past and, his coming guce art trany,\n",
      "an e\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8912], grad_fn=<DivBackward0>) \n",
      "\n",
      "Start string:  b\n",
      "Predicted string: \n",
      "enters?\n",
      "And crook baniss,\n",
      "And to well ser, me withere be I wervirm ow,\n",
      "Ay me?\n",
      "\n",
      "KI VINCES:\n",
      "Thang. \n",
      "Buthou.\n",
      "\n",
      "Prive,\n",
      "And swear and whe sustion. Ged\n",
      "Id oft condanch ne bisgan of exer you may that wit, And\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0126], grad_fn=<DivBackward0>) \n",
      "\n",
      "Start string:  b\n",
      "Predicted string: \n",
      "me to-the all it go dearse bith care! our we wiss the wear pore so lue hare freser the queine me you muck of and come-bighblous, wall stence, freade bare, sulf and of now sursher benen, maling\n",
      "To of n\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8241], grad_fn=<DivBackward0>) \n",
      "\n",
      "Start string:  b\n",
      "Predicted string: \n",
      "ut shaught ewaple foo teland has and soulf loupting shallate hear\n",
      "This beed the shall the for of the remolion in hees of willade.\n",
      "\n",
      "EONTIO:\n",
      "To parcusink the piter by Com love in anis what my come am de\n",
      " ====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    # 랜덤한 텍스트 덩어리를 샘플링하고 이를 인덱스 텐서로 변환합니다. \n",
    "    inp,label = random_training_set()\n",
    "    hidden = rnn.init_hidden()\n",
    "\n",
    "    loss = torch.tensor([0]).type(torch.FloatTensor)\n",
    "    rnn_optimizer.zero_grad()\n",
    "    for j in range(chunk_len-1):\n",
    "        x  = inp[j]\n",
    "        y_ = label[j].unsqueeze(0).type(torch.LongTensor)\n",
    "        y,hidden = rnn(x,hidden)\n",
    "        loss += loss_func(y,y_)\n",
    "\n",
    "    loss.backward()\n",
    "    rnn_optimizer.step()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"\\n\",loss/chunk_len,\"\\n\")\n",
    "        test(rnn)\n",
    "        print(\"\\n\",\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_epochs):\n",
    "    # 랜덤한 텍스트 덩어리를 샘플링하고 이를 인덱스 텐서로 변환합니다. \n",
    "    inp,label = random_training_set()\n",
    "    hidden = rnn_lstm.init_hidden()\n",
    "\n",
    "    loss = torch.tensor([0]).type(torch.FloatTensor)\n",
    "    rnn_lstm_optimizer.zero_grad()\n",
    "    for j in range(chunk_len-1):\n",
    "        x  = inp[j]\n",
    "        y_ = label[j].unsqueeze(0).type(torch.LongTensor)\n",
    "        y,hidden = rnn_lstm(x,hidden)\n",
    "        loss += loss_func(y,y_)\n",
    "\n",
    "    loss.backward()\n",
    "    rnn_lstm_optimizer.step()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"\\n\",loss/chunk_len,\"\\n\")\n",
    "        test(rnn_lstm)\n",
    "        print(\"\\n\",\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
